{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\emiliaadawe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\emiliaadawe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emiliaadawe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "from scipy import stats\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from time import time\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "import statistics\n",
    "import pyodbc\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    \n",
    "    def strip_html_tags(text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        stripped_text = soup.get_text()\n",
    "        return stripped_text\n",
    "\n",
    "    def strip_urls(text):\n",
    "        #url regex\n",
    "        url_re = re.compile(r\"\"\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))\"\"\")\n",
    "        stripped_text = url_re.sub('',text)\n",
    "        return stripped_text\n",
    "\n",
    "    def strip_emails(text):\n",
    "        #email address regex\n",
    "        email_re = re.compile(r'(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)')\n",
    "        stripped_text = email_re.sub('',text)\n",
    "        return stripped_text\n",
    "\n",
    "    def strip_nonsense(text):\n",
    "        # leave words that are at least three characters long, do not contain a number, and are no more \n",
    "        # than 17 chars long\n",
    "        no_nonsense = re.findall(r'\\b[a-z][a-z][a-z]+\\b',text)\n",
    "        stripped_text = ' '.join(w for w in no_nonsense if w != 'nan' and len(w) <= 17)\n",
    "        return stripped_text\n",
    "    \n",
    "    doc = str(doc).lower()\n",
    "    tag_free = strip_html_tags(doc)\n",
    "    url_free = strip_urls(tag_free)\n",
    "    email_free = strip_emails(url_free)\n",
    "    normalized_1 = strip_nonsense(email_free)\n",
    "    \n",
    "    stop_free = \" \".join([i for i in normalized_1.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(WordNetLemmatizer().lemmatize(word) for word in punc_free.split())\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model(x):\n",
    "    \n",
    "    n_samples = 2000\n",
    "    n_features = 1000\n",
    "    n_components = 7\n",
    "    n_top_words = 10\n",
    "    \n",
    "    def print_top_words(model, feature_names, n_top_words):\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            message = \"Topic #%d: \" % topic_idx\n",
    "            message += \", \".join([feature_names[i]\n",
    "                                 for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "            print(message)\n",
    "        print()\n",
    "\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    t0 = time()\n",
    "    data_samples = x\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    # Use tf-idf features for NMF.\n",
    "    print(\"Extracting tf-idf features for NMF...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english',\n",
    "                                    ngram_range  = (1,2))\n",
    "    t0 = time()\n",
    "    tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    # Use tf (raw term count) features for LDA.\n",
    "    print(\"Extracting tf features for LDA...\")\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                   ngram_range = (0,2))\n",
    "    t0 = time()\n",
    "    tf = tf_vectorizer.fit_transform(data_samples)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    # Fit the NMF model\n",
    "    print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (n_samples, n_features))\n",
    "    t0 = time()\n",
    "    nmf = NMF(n_components=n_components, random_state=1,\n",
    "              alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "    # Fit the NMF model\n",
    "    print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "          \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "          % (n_samples, n_features))\n",
    "    t0 = time()\n",
    "    nmf = NMF(n_components=n_components, random_state=1,\n",
    "              beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "              l1_ratio=.5).fit(tfidf)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "    print(\"Fitting LDA models with tf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (n_samples, n_features))\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    t0 = time()\n",
    "    lda.fit(tf)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nTopics in LDA model:\")\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(x):\n",
    "    raw_text = x.tolist()\n",
    "\n",
    "    text_data = []\n",
    "    for text in raw_text:\n",
    "        tokens = clean(text)\n",
    "        text_data.append(tokens)\n",
    "    \n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_to_list (x):\n",
    "    n_samples = 2000\n",
    "    n_features = 1000\n",
    "    n_components = 6\n",
    "    n_top_words = 10\n",
    "    #max_df=0.95, min_df=2,\n",
    "    tf_vectorizer = CountVectorizer(\n",
    "                                token_pattern=r'(?u)\\b\\w+\\b',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                   ngram_range = (1,3))\n",
    "\n",
    "    tf = tf_vectorizer.fit_transform(x)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    lda.fit(tf)\n",
    "    temp_list =[]\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        #message = \"Topic #%d: \" % topic_idx\n",
    "        message = ''\n",
    "        message += \", \".join([tf_feature_names[i]\n",
    "                            for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        temp_list.append(message)\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component  \n",
    "Topic Modeling by Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = pd.read_excel(r'C:\\Users\\emiliaadawe\\Documents\\data\\CSS_Burden_Reduction_2020_v1.xlsx', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = df_comp.drop(['Select your Occupational Category', 'Where do you work?', 'Survey_ID',\n",
    "       'SUB_COMPONENT', 'GRADELEVEL', 'SUP_STATUS', 'In your typical 40 hour week approximately how many hours would you classify as \"low value\" work?',\n",
    "       'How many years have you worked for the Federal Government?',\n",
    "       'Since you have worked for the Federal Government for less than 1 year, please specify the numbers of months you have worked for the Federal Government below.',\n",
    "       'How many years have you worked for your current Agency?',\n",
    "       'Since you have worked for your Agency for less than 1 year, please specify the number of months you have worked for your current agency below.',\n",
    "       'On average how many days per month do you telework?',\n",
    "       'What is the highest degree or level of education you have completed?',\n",
    "       'Please select your age.',\n",
    "       'Are you considering leaving your organization within the next year?'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = df_comp.rename(index=str, columns={\n",
    "        'Please briefly describe an example of one burdensome administrative task or process which you believe is \"low value\"': 'low_value_text'\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGENCY</th>\n",
       "      <th>COMPONENT</th>\n",
       "      <th>low_value_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Department of Agriculture</td>\n",
       "      <td>Cooperative State Research, Education, and Ext...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Department of Agriculture</td>\n",
       "      <td>Cooperative State Research, Education, and Ext...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Department of Agriculture</td>\n",
       "      <td>Cooperative State Research, Education, and Ext...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Department of Agriculture</td>\n",
       "      <td>Cooperative State Research, Education, and Ext...</td>\n",
       "      <td>surveys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Department of Agriculture</td>\n",
       "      <td>Cooperative State Research, Education, and Ext...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      AGENCY  \\\n",
       "0  Department of Agriculture   \n",
       "1  Department of Agriculture   \n",
       "2  Department of Agriculture   \n",
       "3  Department of Agriculture   \n",
       "4  Department of Agriculture   \n",
       "\n",
       "                                           COMPONENT low_value_text  \n",
       "0  Cooperative State Research, Education, and Ext...            NaN  \n",
       "1  Cooperative State Research, Education, and Ext...            NaN  \n",
       "2  Cooperative State Research, Education, and Ext...            NaN  \n",
       "3  Cooperative State Research, Education, and Ext...        surveys  \n",
       "4  Cooperative State Research, Education, and Ext...            NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp['COMPONENT'] = df_comp['COMPONENT'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_comp[df_comp['low_value_text'].isnull()==False]['low_value_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp['COMPONENT'] = df_comp['COMPONENT'].astype(str)\n",
    "df_comp['low_value_text'] = df_comp['low_value_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp =df_comp.loc[df_comp['low_value_text'] != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp['COMPOUND_SENT'] = df_comp['low_value_text'].apply(lambda x: sid.polarity_scores(x)['compound'] if pd.isnull(x)==False else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "useless_words = ['would','could','should','le','non','federal','government','agency','way','low','value','work', 'time','hour','lack','make','lot','like','need','getting','use','day','need','needed','believe','thing','task']\n",
    "exclude = set(string.punctuation) \n",
    "for word in useless_words:\n",
    "    stop.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_agency = df_comp['AGENCY'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_component = df_comp['COMPONENT'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9c47d7f27a9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdf_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_comp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_comp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'COMPONENT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_comp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AGENCY'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0magency\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0msentences_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'low_value_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'low_value_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mreturn_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mreturn_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mreturn_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-25b72e645b20>\u001b[0m in \u001b[0;36mlda_to_list\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     11\u001b[0m                                    ngram_range = (1,3))\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
      "\u001b[1;32mC:\\Program-Files\\Ananconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1220\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program-Files\\Ananconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1148\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m   1151\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "for agency in unique_agency:\n",
    "    for component in unique_component:\n",
    "        temp_list = []\n",
    "        index = df_comp['AGENCY'], df_comp['COMPONENT']\n",
    "        df_temp = df_comp[df_comp['COMPONENT']==component][df_comp['AGENCY']==agency]\n",
    "        sentences_temp = df_temp[df_temp['low_value_text'].isnull()==False]['low_value_text']\n",
    "        return_list = lda_to_list(tokenize_text(sentences_temp))\n",
    "        return_list.append(component)  \n",
    "        return_list.append(len(df_temp))\n",
    "        if return_list is None:\n",
    "            print(component)\n",
    "            break\n",
    "        lda_list.append(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list = lda_to_list(tokenize_text(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list.append('ALL GOV')\n",
    "rand_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_list.append(rand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_temp = ['topic_1','topic_2','topic_3','topic_4','topic_5','topic_6','component','agency','comm_len']\n",
    "df_lda = pd.DataFrame(lda_list,columns =cols_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda['comm_len'] = pd.to_numeric(df_lda['comm_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_long = df_lda[df_lda['comm_len']>25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_melt = pd.melt(df_lda_long,id_vars='component',value_vars = [x for x in list(df_lda_long.columns) if x !='component'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_long.to_excel('Component Topics.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp.to_excel('Component Sentiment_v2.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp_sent = df_comp[['COMPONENT','COMPOUND_SENT']].groupby('COMPONENT').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp_sent.to_excel('Component Sentiment.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sr. Manager \n",
    "Topic Modeling by agency filtered to Supervisors Greater than GS-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr = pd.read_excel(r'C:\\Users\\emiliaadawe\\Documents\\data\\CSS_Burden_Reduction_2020_v1.xlsx', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr = df_mgr.loc[df_mgr['SUP_STATUS'] ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr = df_mgr.drop(['Select your Occupational Category', 'Where do you work?', 'Survey_ID',\n",
    "       'COMPONENT', 'SUB_COMPONENT', 'GRADELEVEL', 'SUP_STATUS', 'In your typical 40 hour week approximately how many hours would you classify as \"low value\" work?',\n",
    "       'How many years have you worked for the Federal Government?',\n",
    "       'Since you have worked for the Federal Government for less than 1 year, please specify the numbers of months you have worked for the Federal Government below.',\n",
    "       'How many years have you worked for your current Agency?',\n",
    "       'Since you have worked for your Agency for less than 1 year, please specify the number of months you have worked for your current agency below.',\n",
    "       'On average how many days per month do you telework?',\n",
    "       'What is the highest degree or level of education you have completed?',\n",
    "       'Please select your age.',\n",
    "       'Are you considering leaving your organization within the next year?'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr = df_mgr.rename(index=str, columns={\n",
    "        'Please briefly describe an example of one burdensome administrative task or process which you believe is \"low value\"': 'low_value_text'\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr['AGENCY'] = df_mgr['AGENCY'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_mgr[df_mgr['low_value_text'].isnull()==False]['low_value_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr['AGENCY'] = df_mgr['AGENCY'].astype(str)\n",
    "df_mgr['low_value_text'] = df_mgr['low_value_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr =df_mgr.loc[df_mgr['low_value_text'] != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr['COMPOUND_SENT'] = df_mgr['low_value_text'].apply(lambda x: sid.polarity_scores(x)['compound'] if pd.isnull(x)==False else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "useless_words = ['would','could','should','le','non','federal','government','agency','way','low','value','work', 'time','hour','lack','make','lot','like','need','getting','use','day','need','needed','believe','thing','task']\n",
    "exclude = set(string.punctuation) \n",
    "for word in useless_words:\n",
    "    stop.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_agency = df_mgr['AGENCY'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agency in unique_agency:\n",
    "    temp_list = []\n",
    "    df_temp = df_mgr[df_mgr['AGENCY']==agency]\n",
    "    sentences_temp = df_temp[df_temp['low_value_text'].isnull()==False]['low_value_text']\n",
    "    return_list = lda_to_list(tokenize_text(sentences_temp))\n",
    "    return_list.append(agency)  \n",
    "    return_list.append(len(df_temp))\n",
    "    if return_list is None:\n",
    "        print(agency)\n",
    "        break\n",
    "    lda_list.append(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list = lda_to_list(tokenize_text(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list.append('ALL GOV')\n",
    "rand_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_list.append(rand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_temp = ['topic_1','topic_2','topic_3','topic_4','topic_5','topic_6','agency','comm_len']\n",
    "df_lda = pd.DataFrame(lda_list,columns =cols_temp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda['comm_len'] = pd.to_numeric(df_lda['comm_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_long = df_lda[df_lda['comm_len']>25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_melt = pd.melt(df_lda_long,id_vars='agency',value_vars = [x for x in list(df_lda_long.columns) if x !='agency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_long.to_excel('Sr. MGR Topics.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr_sent = df_mgr[['AGENCY','COMPOUND_SENT']].groupby('AGENCY').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr_sent.to_excel('Sr. MGR Sentiment.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GS Groups\n",
    "Topic Modeling by GS Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs = pd.read_excel(r'C:\\Users\\emiliaadawe\\Documents\\data\\CSS_Burden_Reduction_2020_v1.xlsx', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs_group(df):\n",
    "    gs_dict = {'GS-1' : 'GS 1-6',\n",
    "                'GS-2' : 'GS 1-6',\n",
    "                'GS-3' : 'GS 1-6',\n",
    "                'GS-4' : 'GS 1-6',\n",
    "                'GS-5' : 'GS 1-6',\n",
    "                'GS-6' : 'GS 1-6',\n",
    "                'GS-7' : 'GS 7-9',\n",
    "                'GS-8' : 'GS 7-9',\n",
    "                'GS-9' : 'GS 7-9',\n",
    "                'GS-10' : 'GS 10-12',\n",
    "                'GS-11' : 'GS 10-12',\n",
    "                'GS-12' : 'GS 10-12',\n",
    "                'GS-13' : 'GS 13',\n",
    "                'GS-14' : 'GS 14',\n",
    "                'GS-15' : 'GS 15',\n",
    "                'Other' : 'Other',\n",
    "                'SES' : 'SES',\n",
    "                'SL' : 'SL & ST',\n",
    "                'ST' : 'SL & ST',\n",
    "                'NULL': None}\n",
    "    \n",
    "    df['GS GROUP'] = df['GRADELEVEL'].map(gs_dict)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs = gs_group(df_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs = df_gs.drop(['Select your Occupational Category', 'Where do you work?', 'Survey_ID',\n",
    "       'COMPONENT', 'SUB_COMPONENT', 'GRADELEVEL', 'SUP_STATUS', 'In your typical 40 hour week approximately how many hours would you classify as \"low value\" work?',\n",
    "       'How many years have you worked for the Federal Government?',\n",
    "       'Since you have worked for the Federal Government for less than 1 year, please specify the numbers of months you have worked for the Federal Government below.',\n",
    "       'How many years have you worked for your current Agency?',\n",
    "       'Since you have worked for your Agency for less than 1 year, please specify the number of months you have worked for your current agency below.',\n",
    "       'On average how many days per month do you telework?',\n",
    "       'What is the highest degree or level of education you have completed?',\n",
    "       'Please select your age.',\n",
    "       'Are you considering leaving your organization within the next year?'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs = df_gs.rename(index=str, columns={\n",
    "        'Please briefly describe an example of one burdensome administrative task or process which you believe is \"low value\"': 'low_value_text'\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_gs[df_gs['low_value_text'].isnull()==False]['low_value_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs['GS GROUP'] = df_gs['GS GROUP'].astype(str)\n",
    "df_gs['low_value_text'] = df_gs['low_value_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs =df_gs.loc[df_gs['low_value_text'] != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs['COMPOUND_SENT'] = df_gs['low_value_text'].apply(lambda x: sid.polarity_scores(x)['compound'] if pd.isnull(x)==False else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "useless_words = ['would','could','should','le','non','federal','government','agency','way','low','value','work', 'time','hour','lack','make','lot','like','need','getting','use','day','need','needed','believe','thing','task']\n",
    "exclude = set(string.punctuation) \n",
    "for word in useless_words:\n",
    "    stop.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_group = df_gs['GS GROUP'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in unique_group:\n",
    "    temp_list = []\n",
    "    df_temp = df_gs[df_gs['GS GROUP']==group]\n",
    "    sentences_temp = df_temp[df_temp['low_value_text'].isnull()==False]['low_value_text']\n",
    "    return_list = lda_to_list(tokenize_text(sentences_temp))\n",
    "    return_list.append(group)  \n",
    "    return_list.append(len(df_temp))\n",
    "    if return_list is None:\n",
    "        print(group)\n",
    "        break\n",
    "    lda_list.append(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list = lda_to_list(tokenize_text(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list.append('ALL GOV')\n",
    "rand_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_list.append(rand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_temp = ['topic_1','topic_2','topic_3','topic_4','topic_5','topic_6','gs group','comm_len']\n",
    "df_lda = pd.DataFrame(lda_list,columns =cols_temp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda['comm_len'] = pd.to_numeric(df_lda['comm_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_long = df_lda[df_lda['comm_len']>25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_melt = pd.melt(df_lda_long,id_vars='gs group',value_vars = [x for x in list(df_lda_long.columns) if x !='gs group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_long.to_excel('GS Group Topics.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs_sent = df_gs[['GS GROUP','COMPOUND_SENT']].groupby('GS GROUP').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs_sent.to_excel('GS Group Sentiment.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Non Sr. Manager \n",
    "Topic Modeling by agency filtered to Supervisors less than GS-13 and non supervisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonmgr = pd.read_excel(r'C:\\Users\\emiliaadawe\\Documents\\data\\CSS_Burden_Reduction_2020_v1.xlsx', encoding='utf-8')\n",
    "\n",
    "df_nonmgr = df_nonmgr.loc[df_nonmgr['SUP_STATUS'] ==0]\n",
    "\n",
    "df_nonmgr = df_nonmgr.drop(['Select your Occupational Category', 'Where do you work?', 'Survey_ID',\n",
    "       'COMPONENT','SUB_COMPONENT', 'GRADELEVEL', 'SUP_STATUS', 'In your typical 40 hour week approximately how many hours would you classify as \"low value\" work?',\n",
    "       'How many years have you worked for the Federal Government?',\n",
    "       'Since you have worked for the Federal Government for less than 1 year, please specify the numbers of months you have worked for the Federal Government below.',\n",
    "       'How many years have you worked for your current Agency?',\n",
    "       'Since you have worked for your Agency for less than 1 year, please specify the number of months you have worked for your current agency below.',\n",
    "       'On average how many days per month do you telework?',\n",
    "       'What is the highest degree or level of education you have completed?',\n",
    "       'Please select your age.',\n",
    "       'Are you considering leaving your organization within the next year?'], axis=1)\n",
    "\n",
    "df_nonmgr = df_nonmgr.rename(index=str, columns={\n",
    "        'Please briefly describe an example of one burdensome administrative task or process which you believe is \"low value\"': 'low_value_text'\n",
    "        })\n",
    "\n",
    "df_nonmgr['AGENCY'] = df_nonmgr['AGENCY'].str.upper()\n",
    "\n",
    "sentences = df_nonmgr[df_nonmgr['low_value_text'].isnull()==False]['low_value_text']\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_nonmgr['AGENCY'] = df_nonmgr['AGENCY'].astype(str)\n",
    "df_nonmgr['low_value_text'] = df_nonmgr['low_value_text'].astype(str)\n",
    "\n",
    "df_nonmgr =df_nonmgr.loc[df_nonmgr['low_value_text'] != 'nan']\n",
    "\n",
    "df_nonmgr['COMPOUND_SENT'] = df_nonmgr['low_value_text'].apply(lambda x: sid.polarity_scores(x)['compound'] if pd.isnull(x)==False else None)\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "useless_words = ['would','could','should','le','non','federal','government','agency','way','low','value','work', 'time','hour','lack','make','lot','like','need','getting','use','day','need','needed','believe','thing','task']\n",
    "exclude = set(string.punctuation) \n",
    "for word in useless_words:\n",
    "    stop.add(word)\n",
    "\n",
    "unique_agency = df_nonmgr['AGENCY'].unique()\n",
    "\n",
    "lda_list = []\n",
    "\n",
    "for agency in unique_agency:\n",
    "    temp_list = []\n",
    "    df_temp = df_nonmgr[df_nonmgr['AGENCY']==agency]\n",
    "    sentences_temp = df_temp[df_temp['low_value_text'].isnull()==False]['low_value_text']\n",
    "    return_list = lda_to_list(tokenize_text(sentences_temp))\n",
    "    return_list.append(agency)  \n",
    "    return_list.append(len(df_temp))\n",
    "    if return_list is None:\n",
    "        print(agency)\n",
    "        break\n",
    "    lda_list.append(return_list)\n",
    "\n",
    "rand_list = lda_to_list(tokenize_text(sentences))\n",
    "\n",
    "rand_list.append('ALL GOV')\n",
    "rand_list.append(0)\n",
    "\n",
    "lda_list.append(rand_list)\n",
    "\n",
    "cols_temp = ['topic_1','topic_2','topic_3','topic_4','topic_5','topic_6','agency','comm_len']\n",
    "df_lda = pd.DataFrame(lda_list,columns =cols_temp )\n",
    "\n",
    "df_lda['comm_len'] = pd.to_numeric(df_lda['comm_len'])\n",
    "\n",
    "df_lda_long = df_lda[df_lda['comm_len']>25]\n",
    "\n",
    "df_lda_melt = pd.melt(df_lda_long,id_vars='agency',value_vars = [x for x in list(df_lda_long.columns) if x !='agency'])\n",
    "\n",
    "df_lda_long.to_excel('Non MGR Topics.xlsx', index=False)\n",
    "\n",
    "df_nonmgr_sent = df_nonmgr[['AGENCY','COMPOUND_SENT']].groupby('AGENCY').mean()\n",
    "\n",
    "df_nonmgr_sent.to_excel('Non MGR Sentiment.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
